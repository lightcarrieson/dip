{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa413c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (TextClassificationPipeline,\n",
    "                          RobertaTokenizerFast, RobertaForSequenceClassification)\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27de53",
   "metadata": {},
   "source": [
    "Initialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_model_path = ...\n",
    "discourse_model_path = ...\n",
    "grammar_model_path = ...\n",
    "lexical_model_path = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "general = TextClassificationPipeline(\n",
    "    model=RobertaForSequenceClassification.from_pretrained(general_model_path),\n",
    "    tokenizer=RobertaTokenizerFast.from_pretrained(general_model_path),\n",
    "    top_k=None\n",
    ")\n",
    "\n",
    "discourse = TextClassificationPipeline(\n",
    "    model=RobertaForSequenceClassification.from_pretrained(discourse_model_path),\n",
    "    tokenizer=RobertaTokenizerFast.from_pretrained(discourse_model_path),\n",
    ")\n",
    "\n",
    "grammar = TextClassificationPipeline(\n",
    "    model=RobertaForSequenceClassification.from_pretrained(grammar_model_path),\n",
    "    tokenizer=RobertaTokenizerFast.from_pretrained(grammar_model_path),\n",
    ")\n",
    "\n",
    "lexical = TextClassificationPipeline(\n",
    "    model=RobertaForSequenceClassification.from_pretrained(lexical_model_path),\n",
    "    tokenizer=RobertaTokenizerFast.from_pretrained(lexical_model_path),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8334335c",
   "metadata": {},
   "source": [
    "Fetch data to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bdc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open(path/to/train, 'rb'))\n",
    "\n",
    "data = pd.DataFrame(train, columns=['text', 'label'])\n",
    "dataset = Dataset.from_pandas(data[['text', 'label']])\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "df = dataset['test'].to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, labels = df['text'].tolist(), df['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404fc50f",
   "metadata": {},
   "source": [
    "In the original Grammar model, there's an issue where the id2label does not match actual values. This dictionary was introduced to adjust the model's predictions to the actual classes; if the model is trained from anew, however, there is no need for the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30413db",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = {\n",
    " 'Verb_pattern': 'Verb_pattern',\n",
    " 'Confusion_of_structures': 'Confusion_of_structures',\n",
    " 'Comparison_degree': 'Voice',\n",
    " 'Formational_affixes': 'Comparison_degree',\n",
    " 'Prepositions': 'Formational_affixes',\n",
    " 'Category_confusion': 'Prepositions',\n",
    " 'Agreement_errors': 'Category_confusion',\n",
    " 'Numerals': 'Agreement_errors',\n",
    " 'Tense_form': 'Numerals',\n",
    " 'Voice': 'Relative_clause',\n",
    " 'Relative_clause': 'Tense_form'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b6eed",
   "metadata": {},
   "source": [
    "Predict general classes; then, for 'uncertain' predictions make 3 predictions and remember 3 scores, for 'certain' predictions just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "certain_answers = []\n",
    "uncertain_answers = []\n",
    "\n",
    "for n, sent in enumerate(tqdm(sents)):\n",
    "    general_class = general(sent)[0]\n",
    "    general_label = general_class[0]['label']\n",
    "    \n",
    "    # certainty threshold: 0.85\n",
    "    if general_class[0]['score'] > 0.85:\n",
    "        if general_label[0] == 'l':\n",
    "            pred = lexical(sent)[0]\n",
    "            label, score = pred['label'], pred['score']\n",
    "        elif general_label[0] == 'd':\n",
    "            pred = discourse(sent)[0]\n",
    "            label, score = pred['label'], pred['score']\n",
    "        else:\n",
    "            pred = grammar(sent)[0]\n",
    "            label, score = real[pred['label']], pred['score']\n",
    "            \n",
    "        certain_answers.append((sent, labels[n], general_label, general_class[0]['score'], label, score))\n",
    "        \n",
    "    else:\n",
    "        lex = lexical(sent)[0]\n",
    "        disc = discourse(sent)[0]\n",
    "        gram = grammar(sent)[0]\n",
    "        \n",
    "        general_classes = {i['label']:i['score'] for i in general_class}\n",
    "        \n",
    "        uncertain_answers.append((sent, labels[n],\n",
    "                                general_classes['lexical'],\n",
    "                                general_classes['discourse'],\n",
    "                                general_classes['gram'],\n",
    "                                lex['label'], lex['score'],\n",
    "                                disc['label'], disc['score'],\n",
    "                                real[gram['label']], gram['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25129307",
   "metadata": {},
   "outputs": [],
   "source": [
    "certain = pd.DataFrame(certain_answers, columns=['error', 'target', 'general', 'general score', 'predicted', 'score'])\n",
    "uncertain = pd.DataFrame(uncertain_answers, columns=['error', 'target',\n",
    "                                                    'lexical gen', 'discourse gen', 'grammar gen',\n",
    "                                                    'lexical pred', 'lexical score',\n",
    "                                                    'discourse pred', 'discourse score',\n",
    "                                                    'grammar pred', 'grammar score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d57107",
   "metadata": {},
   "source": [
    "For uncertain errors, choose the best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b246a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain['disc likelihood'] = uncertain['discourse gen'] * uncertain['discourse score']\n",
    "uncertain['grammar likelihood'] = uncertain['grammar gen'] * uncertain['grammar score']\n",
    "uncertain['lexical likelihood'] = uncertain['lexical gen'] * uncertain['lexical score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c89d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain['candidate class'] = uncertain[['disc likelihood', 'grammar likelihood', 'lexical likelihood']].idxmax(axis=1)\n",
    "uncertain['candidate prob'] = uncertain[['disc likelihood', 'grammar likelihood', 'lexical likelihood']].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate(r):\n",
    "    \n",
    "    if r['candidate class'][0] == 'd':\n",
    "        return r['discourse pred']\n",
    "    elif r['candidate class'][0] == 'g':\n",
    "        return r['grammar pred']\n",
    "    else:\n",
    "        return r['lexical pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain['predicted'] = uncertain.apply(candidate, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420a83e",
   "metadata": {},
   "source": [
    "Filter out predictions we're not sure in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "certain_threshold = 0.7\n",
    "uncertain_threshold = 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain['accepted'] = uncertain.apply(lambda x: x['candidate prob'] > uncertain_threshold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b438fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "certain['accepted'] = certain.apply(lambda x: x['score'] > certain_threshold, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d56c33",
   "metadata": {},
   "source": [
    "Save predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5471a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain.to_excel('uncertain.xlsx')\n",
    "certain.to_excel('certain.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff32b38",
   "metadata": {},
   "source": [
    "Two files are saved: in both, the column called 'predicted' contains the prediction. The column called 'accepted' contains a Boolean value on whether it passes the respective threshold or now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
